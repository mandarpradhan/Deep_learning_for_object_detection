{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CONFIG import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Our_VGG16 import vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = vgg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.classifier = nn.Sequential(nn.Linear(512*7*7,4096),nn.ReLU(True),nn.Dropout(),nn.Linear(4096,1470))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load('best.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vgg(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace)\n",
       "    (4): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace)\n",
       "    (9): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace)\n",
       "    (16): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace)\n",
       "    (23): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace)\n",
       "    (30): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Dropout(p=0.5)\n",
       "    (3): Linear(in_features=4096, out_features=1470, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor(net,img_name,root_path = ''):\n",
    "    img = cv2.imread(root_path+img_name)\n",
    "    height,width,channels = img.shape\n",
    "    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "    mean = np.mean(img,axis = (0,1))\n",
    "    img = img - mean\n",
    "    img = cv2.resize(img,(224,224))\n",
    "    t = transforms.Compose([transforms.ToTensor()])\n",
    "    img = t(img)\n",
    "    print(img.shape)\n",
    "    img = Variable(img[None,:,:,:], volatile = True) #Volatile = True is not done here, try it if things aren't working\n",
    "    print(img.shape)\n",
    "    img = img.cuda()\n",
    "    prediction = net(img)\n",
    "    prediction.cpu()\n",
    "    predictions = []\n",
    "    prediction = prediction.data.squeeze(0)\n",
    "    bb1_confidence = prediction[:,:,4].unsqueeze(2)\n",
    "    bb2_confidence = prediction[:,:,9].unsqueeze(2)\n",
    "    both_confidences = torch.cat((bb1_confidence,bb2_confidence),2)\n",
    "    if both_confidences.max() < confidence_threshold:\n",
    "        bb_in_cell = both_confidences==both_confidences.max()\n",
    "    else:\n",
    "        bb_in_cell = both_confidences>confidence_threshold\n",
    "    class_probabilities = []\n",
    "    bounding_boxes = []\n",
    "    class_indexes = []\n",
    "    bbox_confidence = []\n",
    "    for i in range(cells_per_row):\n",
    "        for j in range(cells_per_row):\n",
    "            cell_data = prediction[i,j,:]\n",
    "#             print \"cell_data type : \",type(cell_data)\n",
    "            if bb_in_cell[i,j,0] == 1:\n",
    "                bbox_1 = cell_data[:4]\n",
    "                bbox_1.cpu()\n",
    "                bbox_1_confidence = torch.FloatTensor([cell_data[4]])\n",
    "                top_left = torch.FloatTensor([j,i])*cell_size\n",
    "#                 bbox_1[:2] = bbox_1[:2]*cell_size + top_left\n",
    "                print(\"(bbox_1[:2])\",type(bbox_1[:2]),len(bbox_1[:2]))\n",
    "                print(\"(bbox_1[:2]*cell_size)\",type(bbox_1[:2]*cell_size),len(bbox_1[:2]*cell_size))\n",
    "                print(\"top_left :\",type(top_left),len(top_left))\n",
    "                print(bbox_1[:2])\n",
    "                bbox_1[:2] = bbox_1[:2]*cell_size + top_left\n",
    "                bbox_1_xy = torch.FloatTensor(bbox_1.size())\n",
    "                bbox_1_xy[:2] = bbox_1[:2] - (bbox_1[2:]/2)\n",
    "                bbox_1_xy[2:] = bbox_1[:2] + (bbox_1[2:]/2)\n",
    "                print(\"len(cell_data[10:]) :\",len(cell_data[10:]))\n",
    "                highest_prob,class_index = torch.max(cell_data[10:],0)\n",
    "                bounding_boxes.append(bbox_1_xy.view(1,4))\n",
    "                class_indexes.append(class_index)\n",
    "                class_probabilities.append(highest_prob)\n",
    "                bbox_confidence.append(bbox_1_confidence)\n",
    "            elif bb_in_cell[i,j,1] == 1:\n",
    "                bbox_2 = cell_data[5:9]\n",
    "                bbox_2_confidence = torch.FloatTensor([cell_data[9]])\n",
    "                top_left = torch.FloatTensor([j,i])*cell_size\n",
    "                bbox_2[:2] = bbox_2[:2]*cell_size + top_left\n",
    "                bbox_2_xy = torch.FloatTensor(bbox_2.size())\n",
    "                bbox_2_xy[:2] = bbox_2[:2] - (bbox_2[2:]/2)\n",
    "                bbox_2_xy[2:] = bbox_2[:2] + (bbox_2[2:]/2)\n",
    "                highest_prob,class_index = torch.max(cell_data[10:],0)\n",
    "                bounding_boxes.append(bbox_2_xy.view(1,4))\n",
    "                class_indexes.append(class_index)\n",
    "                class_probabilities.append(highest_prob)\n",
    "                bbox_confidence.append(bbox_2_confidence)\n",
    "    bounding_boxes = torch.cat(bounding_boxes,0)\n",
    "    bbox_confidence = torch.cat(bbox_confidence,0)\n",
    "    class_indexes = torch.cat(class_indexes,0)\n",
    "    \n",
    "    for i,box in enumerate(bounding_boxes):\n",
    "        x1 = int(box[0]*width)\n",
    "        y1 = int(box[1]*height)\n",
    "        x2 = int(box[2]*width)\n",
    "        y2 = int(box[3]*height)\n",
    "        c = int(class_indexes[i])\n",
    "        p = float(bbox_confidence[i])\n",
    "        predictions.append([(x1,y1),(x2,y2),VOC_CLASSES[c],image_name,p])\n",
    "    \n",
    "    return predictions\n",
    "        \n",
    "                \n",
    "                \n",
    "            \n",
    "    \n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOC_Datasets/VOC_2012_training/VOC2012/JPEGImages/2008_008683.jpg\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "('(bbox_1[:2])', <class 'torch.cuda.FloatTensor'>, 2)\n",
      "('(bbox_1[:2]*cell_size)', <class 'torch.cuda.FloatTensor'>, 2)\n",
      "('top_left :', <class 'torch.FloatTensor'>, 2)\n",
      "\n",
      " 0.7622\n",
      " 0.7615\n",
      "[torch.cuda.FloatTensor of size 2 (GPU 0)]\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "add received an invalid combination of arguments - got (torch.FloatTensor), but expected one of:\n * (float value)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mtorch.FloatTensor\u001b[0m)\n * (torch.cuda.FloatTensor other)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mtorch.FloatTensor\u001b[0m)\n * (torch.cuda.sparse.FloatTensor other)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mtorch.FloatTensor\u001b[0m)\n * (float value, torch.cuda.FloatTensor other)\n * (float value, torch.cuda.sparse.FloatTensor other)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-aad466b750a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mleft_up\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mright_bottom\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrectangle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mleft_up\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mright_bottom\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-72-2791c2e654f9>\u001b[0m in \u001b[0;36mpredictor\u001b[0;34m(net, img_name, root_path)\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"top_left :\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_left\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_left\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                 \u001b[0mbbox_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbbox_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcell_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtop_left\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m                 \u001b[0mbbox_1_xy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0mbbox_1_xy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbbox_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbbox_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python2.7/site-packages/torch/tensor.pyc\u001b[0m in \u001b[0;36m__add__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;31m# TODO: add tests for operators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__add__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m     \u001b[0m__radd__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__add__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: add received an invalid combination of arguments - got (torch.FloatTensor), but expected one of:\n * (float value)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mtorch.FloatTensor\u001b[0m)\n * (torch.cuda.FloatTensor other)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mtorch.FloatTensor\u001b[0m)\n * (torch.cuda.sparse.FloatTensor other)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mtorch.FloatTensor\u001b[0m)\n * (float value, torch.cuda.FloatTensor other)\n * (float value, torch.cuda.sparse.FloatTensor other)\n"
     ]
    }
   ],
   "source": [
    "root_dir = 'VOC_Datasets/VOC_2012_training/VOC2012/JPEGImages/'\n",
    "img_names = os.listdir(root_dir)\n",
    "for i,name in enumerate(img_names):\n",
    "    if i < 15:\n",
    "        pass\n",
    "    else:\n",
    "        image_name = root_dir+name\n",
    "        print(image_name)\n",
    "        image = cv2.imread(image_name)\n",
    "        result = predictor(net,image_name)\n",
    "        for left_up,right_bottom,class_name,_,prob in result:\n",
    "            cv2.rectangle(image,left_up,right_bottom,(0,255,0),2)\n",
    "            cv2.putText(image,class_name,left_up,cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,255),1,2)\n",
    "            print(prob)\n",
    "\n",
    "        cv2.imwrite('train_result'+str(i)+'.jpg',image)\n",
    "    if i > 30:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "if torch.cuda.is_available():\n",
    "    net.cuda()\n",
    "\n",
    "    \n",
    "test_img = cv2.imread('Cat.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(177, 284, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_img = cv2.imread('Cat.jpg')\n",
    "test_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = cv2.cvtColor(test_img,cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([191.53859314, 177.46560436, 170.1299435 ])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(test_img,axis =(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
