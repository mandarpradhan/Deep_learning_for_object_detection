{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding:utf-8\n",
    "#\n",
    "#created by xiongzihua\n",
    "#\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "\n",
    "from net import vgg16\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOC_CLASSES = (    # always index 0\n",
    "    'aeroplane', 'bicycle', 'bird', 'boat',\n",
    "    'bottle', 'bus', 'car', 'cat', 'chair',\n",
    "    'cow', 'diningtable', 'dog', 'horse',\n",
    "    'motorbike', 'person', 'pottedplant',\n",
    "    'sheep', 'sofa', 'train', 'tvmonitor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(pred):\n",
    "    '''\n",
    "    pred (tensor) 1x7x7x30\n",
    "    return (tensor) box[[x1,y1,x2,y2]] label[...]\n",
    "    '''\n",
    "    boxes=[]\n",
    "    cls_indexs=[]\n",
    "    probs = []\n",
    "    cell_size = 1./7\n",
    "    pred = pred.data\n",
    "    pred = pred.squeeze(0) #7x7x30\n",
    "    contain1 = pred[:,:,4].unsqueeze(2)\n",
    "    contain2 = pred[:,:,9].unsqueeze(2)\n",
    "    contain = torch.cat((contain1,contain2),2)\n",
    "    mask1 = contain > 0.7 #大于阈值\n",
    "    mask2 = (contain==contain.max()) #we always select the best contain_prob what ever it>0.9\n",
    "    mask = (mask1+mask2).gt(0)\n",
    "    min_score,min_index = torch.min(mask,2) #每个cell只选最大概率的那个预测框\n",
    "    for i in range(7):\n",
    "        for j in range(7):\n",
    "            for b in range(2):\n",
    "                index = min_index[i,j]\n",
    "#                 mask[i,j,index] = 0\n",
    "                if mask[i,j,b] == 1:\n",
    "                    #print(i,j,b)\n",
    "                    box = pred[i,j,b*5:b*5+4]\n",
    "                    contain_prob = torch.FloatTensor([pred[i,j,b*5+4]])\n",
    "                    xy = torch.FloatTensor([j,i])*cell_size #cell左上角  up left of cell\n",
    "                    box[:2] = box[:2]*cell_size + xy # return cxcy relative to image\n",
    "                    box_xy = torch.FloatTensor(box.size())#转换成xy形式    convert[cx,cy,w,h] to [x1,xy1,x2,y2]\n",
    "                    box_xy[:2] = box[:2] - 0.5*box[2:]\n",
    "                    box_xy[2:] = box[:2] + 0.5*box[2:]\n",
    "                    max_prob,cls_index = torch.max(pred[i,j,10:],0)\n",
    "                    boxes.append(box_xy.view(1,4))\n",
    "                    cls_indexs.append(cls_index)\n",
    "                    probs.append(contain_prob)\n",
    "    boxes = torch.cat(boxes,0) #(n,4)\n",
    "    probs = torch.cat(probs,0) #(n,)\n",
    "    cls_indexs = torch.cat(cls_indexs,0) #(n,)\n",
    "    keep = nms(boxes,probs)\n",
    "    return boxes[keep],cls_indexs[keep],probs[keep]\n",
    "#     return boxes,cls_indexs,probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms(bboxes,scores,threshold=0.5):\n",
    "    '''\n",
    "    bboxes(tensor) [N,4]\n",
    "    scores(tensor) [N,]\n",
    "    '''\n",
    "    x1 = bboxes[:,0]\n",
    "    y1 = bboxes[:,1]\n",
    "    x2 = bboxes[:,2]\n",
    "    y2 = bboxes[:,3]\n",
    "    areas = (x2-x1) * (y2-y1)\n",
    "\n",
    "    _,order = scores.sort(0,descending=True)\n",
    "    keep = []\n",
    "    while order.numel() > 0:\n",
    "        i = order[0]\n",
    "        keep.append(i)\n",
    "\n",
    "        if order.numel() == 1:\n",
    "            break\n",
    "\n",
    "        xx1 = x1[order[1:]].clamp(min=x1[i])\n",
    "        yy1 = y1[order[1:]].clamp(min=y1[i])\n",
    "        xx2 = x2[order[1:]].clamp(max=x2[i])\n",
    "        yy2 = y2[order[1:]].clamp(max=y2[i])\n",
    "\n",
    "        w = (xx2-xx1).clamp(min=0)\n",
    "        h = (yy2-yy1).clamp(min=0)\n",
    "        inter = w*h\n",
    "\n",
    "        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n",
    "        ids = (ovr<=threshold).nonzero().squeeze()\n",
    "        if ids.numel() == 0:\n",
    "            break\n",
    "        order = order[ids+1]\n",
    "    return torch.LongTensor(keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#start predict one image\n",
    "#\n",
    "def predict_gpu(model,image_name,root_path=''):\n",
    "\n",
    "    result = []\n",
    "    image = cv2.imread(root_path+image_name)\n",
    "    h,w,_ = image.shape\n",
    "    img = cv2.resize(image,(224,224))\n",
    "    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "    mean = (123,117,104)#RGB\n",
    "    img = img - np.array(mean,dtype=np.float32)\n",
    "\n",
    "    transform = transforms.Compose([transforms.ToTensor(),])\n",
    "    img = transform(img)\n",
    "    img = Variable(img[None,:,:,:],volatile=True)\n",
    "    img = img.cuda()\n",
    "\n",
    "    pred = model(img) #1x7x7x30\n",
    "    pred = pred.cpu()\n",
    "    boxes,cls_indexs,probs =  decoder(pred)\n",
    "\n",
    "    for i,box in enumerate(boxes):\n",
    "        x1 = int(box[0]*w)\n",
    "        x2 = int(box[2]*w)\n",
    "        y1 = int(box[1]*h)\n",
    "        y2 = int(box[3]*h)\n",
    "        cls_index = cls_indexs[i]\n",
    "        cls_index = int(cls_index) # convert LongTensor to int\n",
    "        prob = probs[i]\n",
    "        prob = float(prob)\n",
    "        result.append([(x1,y1),(x2,y2),VOC_CLASSES[cls_index],image_name,prob])\n",
    "    return result\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = vgg16(pretrained=False)\n",
    "    model.classifier = nn.Sequential(\n",
    "                nn.Linear(512 * 7 * 7, 4096),\n",
    "                nn.ReLU(True),\n",
    "                nn.Dropout(),\n",
    "                #nn.Linear(4096, 4096),\n",
    "                #nn.ReLU(True),\n",
    "                #nn.Dropout(),\n",
    "                nn.Linear(4096, 1470),\n",
    "            )\n",
    "    model.load_state_dict(torch.load('best.pth'))\n",
    "    model.eval()\n",
    "    model.cuda()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.713312923908\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = cv2.imread('Cat.jpg')\n",
    "result = predict_gpu(model,'Cat.jpg')\n",
    "for left_up,right_bottom,class_name,_,prob in result:\n",
    "    cv2.rectangle(i,left_up,right_bottom,(0,255,0),2)\n",
    "    cv2.putText(i,class_name,left_up,cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,255),1,2)\n",
    "    print(prob)\n",
    "\n",
    "cv2.imwrite('Cat_Result_NO_NMS.jpg',i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOC_Datasets/VOC_2012_training/VOC2012/JPEGImages/2008_008683.jpg\n",
      "0.987594485283\n",
      "0.947912335396\n",
      "0.941967189312\n",
      "0.883013606071\n",
      "0.87505710125\n",
      "VOC_Datasets/VOC_2012_training/VOC2012/JPEGImages/2010_002763.jpg\n",
      "0.871656537056\n",
      "0.765147686005\n",
      "0.753469884396\n",
      "VOC_Datasets/VOC_2012_training/VOC2012/JPEGImages/2011_003534.jpg\n",
      "0.887986421585\n",
      "VOC_Datasets/VOC_2012_training/VOC2012/JPEGImages/2011_001600.jpg\n",
      "0.963555455208\n",
      "0.832829594612\n",
      "0.822455108166\n",
      "0.753276467323\n",
      "0.70083373785\n",
      "VOC_Datasets/VOC_2012_training/VOC2012/JPEGImages/2010_005309.jpg\n",
      "0.952035307884\n",
      "VOC_Datasets/VOC_2012_training/VOC2012/JPEGImages/2008_007390.jpg\n",
      "0.766653835773\n",
      "VOC_Datasets/VOC_2012_training/VOC2012/JPEGImages/2008_002369.jpg\n",
      "0.991786420345\n",
      "VOC_Datasets/VOC_2012_training/VOC2012/JPEGImages/2008_004522.jpg\n",
      "0.89200937748\n",
      "VOC_Datasets/VOC_2012_training/VOC2012/JPEGImages/2008_004950.jpg\n",
      "0.93240404129\n",
      "VOC_Datasets/VOC_2012_training/VOC2012/JPEGImages/2008_008545.jpg\n",
      "0.97771191597\n",
      "VOC_Datasets/VOC_2012_training/VOC2012/JPEGImages/2012_004115.jpg\n",
      "0.931306481361\n",
      "VOC_Datasets/VOC_2012_training/VOC2012/JPEGImages/2011_002715.jpg\n",
      "0.837878465652\n",
      "VOC_Datasets/VOC_2012_training/VOC2012/JPEGImages/2008_005362.jpg\n",
      "0.999000370502\n",
      "0.715698182583\n",
      "VOC_Datasets/VOC_2012_training/VOC2012/JPEGImages/2008_005414.jpg\n",
      "0.962950110435\n",
      "0.838318765163\n",
      "0.782003223896\n",
      "VOC_Datasets/VOC_2012_training/VOC2012/JPEGImages/2007_006560.jpg\n",
      "0.910496354103\n",
      "0.820667564869\n",
      "VOC_Datasets/VOC_2012_training/VOC2012/JPEGImages/2011_002063.jpg\n",
      "0.86259406805\n",
      "0.797773778439\n",
      "VOC_Datasets/VOC_2012_training/VOC2012/JPEGImages/2009_004643.jpg\n",
      "0.978141129017\n",
      "0.95181530714\n",
      "0.92190104723\n",
      "0.808110415936\n",
      "0.801507532597\n",
      "0.754700779915\n"
     ]
    }
   ],
   "source": [
    "root_dir = 'VOC_Datasets/VOC_2012_training/VOC2012/JPEGImages/'\n",
    "img_names = os.listdir(root_dir)\n",
    "for i,name in enumerate(img_names):\n",
    "    if i < 15:\n",
    "        pass\n",
    "    else:\n",
    "        image_name = root_dir+name\n",
    "        print(image_name)\n",
    "        image = cv2.imread(image_name)\n",
    "        result = predict_gpu(model,image_name)\n",
    "        for left_up,right_bottom,class_name,_,prob in result:\n",
    "            cv2.rectangle(image,left_up,right_bottom,(0,255,0),2)\n",
    "            cv2.putText(image,class_name,left_up,cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,255),1,2)\n",
    "            print(prob)\n",
    "\n",
    "        cv2.imwrite('train_result'+str(i)+'.jpg',image)\n",
    "    if i > 30:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace)\n",
       "    (4): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace)\n",
       "    (9): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace)\n",
       "    (16): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace)\n",
       "    (23): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace)\n",
       "    (30): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Dropout(p=0.5)\n",
       "    (3): Linear(in_features=4096, out_features=1470, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = vgg16(pretrained=False)\n",
    "model.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            #nn.Linear(4096, 4096),\n",
    "            #nn.ReLU(True),\n",
    "            #nn.Dropout(),\n",
    "            nn.Linear(4096, 1470),\n",
    "        )\n",
    "model.load_state_dict(torch.load('best.pth'))\n",
    "model.eval()\n",
    "model.cuda()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
