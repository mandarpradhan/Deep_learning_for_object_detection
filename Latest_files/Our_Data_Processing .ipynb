{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import sys\n",
    "import os.path\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "from CONFIG import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class yolo_data_preprocessing(data.Dataset):\n",
    "    def __init__(self,img_dir,ann_dir,train,transform):\n",
    "        self.img_dir = img_dir\n",
    "        self.ann_dir = ann_dir\n",
    "        self.train = train\n",
    "        self.transform = transform \n",
    "        \n",
    "        with open(ann_dir) as ann_file:\n",
    "            content = f.readlines()\n",
    "            \n",
    "        self.image_names = []\n",
    "        tracking_num_bboxes = 0\n",
    "        self.boundingboxes = []\n",
    "        self.class_labels = []\n",
    "        self.num_images = 0\n",
    "        \n",
    "        for line in content:\n",
    "            bbox_ = []\n",
    "            class_lbl = []            \n",
    "            self.num_images = self.num_images + 1\n",
    "            line = line.strip().split()\n",
    "            self.image_names += line[0]\n",
    "            tracking_num_bboxes = line[1]\n",
    "            nums = arange(tracking_num_bboxes)\n",
    "            bbox_content = line[3:]    \n",
    "            \n",
    "            for i in nums:\n",
    "                bbox_.append([float(bbox_content[i*num_params]),float(bbox_content[i*num_params+1]),float(bbox_content[i*num_params+2]),float(bbox_content[i*num_params+3])])\n",
    "                class_lbl.append(int(bbox_content[i*num_params+4]))\n",
    "                \n",
    "            bbox_ = Torch.Tensor(bbox_)\n",
    "            class_lbl = Torch.LongTensor(class_lbl)\n",
    "            self.boundingboxes.append(bbox_)\n",
    "            self.class_labels.append(class_lbl)\n",
    "        \n",
    "    def __getitem__(self,img_index):\n",
    "        image = cv2.imread(os.path.join(self.img_dir+self.image_names[img_index]))\n",
    "        width,height,depth = image.shape\n",
    "        img_boxes = self.boundingboxes[img_index] / torch.Tensor([width,height,width,height]).expand_as(self.boundingboxes[0])\n",
    "        img_labels = self.class_labels[img_index]\n",
    "        image = process_image(image)\n",
    "        ground_truth = torch.zeros([cells_per_row,cells_per_row])\n",
    "        width = img_boxes[:,2] - img_boxes[:,0]\n",
    "        height = img_boxes[:,3] - img_boxes[:,1]\n",
    "        width_height = img_boxes[:,2:] - img_boxes[:,:2]\n",
    "        img_center_x = (img_boxes[:,0] + img_boxes[:,2])/2\n",
    "        img_center_y = (img_boxes[:,1] + img_boxes[:,3])/2\n",
    "        for i in range(img_center_x.size()):\n",
    "            first = centers[i]*cells_per_row\n",
    "            coords = first.ceil() - 1\n",
    "            coord_1 = int(coords[1])\n",
    "            coord_2 = int(coords[0])\n",
    "            ground_truth[coord_1,coord_2,conf1_index] = 1\n",
    "            ground_truth[coord_1,coord_2,conf2_index] = 1\n",
    "            ground_truth[coord_1,coord_2,int(self.class_labels[i])+conf2_index] = 1\n",
    "            scaled_coords = coords/7.0 \n",
    "            scaled = (first - scaled_coords)*cells_per_row\n",
    "            ground_truth[coord_1,coord_2,2:4] = width_height[i]\n",
    "            ground_truth[coord_1,coord_2,:2] = scaled\n",
    "            ground_truth[coord_1,coord_2,7:9] = width_height[i]\n",
    "            ground_truth[coord_1,coord_2,5:7] = scaled\n",
    "        self.target_predictions = ground_truth\n",
    "            \n",
    "    def process_image(image):\n",
    "        image = self.BGR2RGB(image)\n",
    "        image = self.subMean(image,self.mean)\n",
    "        image = cv2.resize(image,(size_image,size_image))\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
